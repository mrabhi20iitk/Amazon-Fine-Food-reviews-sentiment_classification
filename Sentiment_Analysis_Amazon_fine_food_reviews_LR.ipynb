{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZqF6Nt7UC6G"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xr5W1mP7UC6I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "ee49f002-bd1e-4746-8d1e-e3e5c8f4fe77"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imblearn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9707db98cdac>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3 as sq\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import SnowballStemmer\n",
        "from tqdm import tqdm    # for progress bar\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score , confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim , gensim.downloader as api      #for downloading google w2v dataset\n",
        "from gensim.models import word2vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lAe-nDWUC6L"
      },
      "source": [
        "Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjr29J8LUC6L"
      },
      "outputs": [],
      "source": [
        "# conn = sq.connect(\"/content/database.sqlite\")\n",
        "\n",
        "# # to identify object names present in SQLite DB\n",
        "# cursor = conn.cursor()\n",
        "# query  = \"SELECT name from sqlite_master\"\n",
        "# cursor.execute(query)\n",
        "# cursor.fetchall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS218dKOUC6M"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_sql_query('SELECT * FROM REVIEWS', conn)\n",
        "data = pd.read_csv('/content/drive/MyDrive/Amazon-Fine-Food-Reviews.csv')\n",
        "\n",
        "print(data.head(3))\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5hWTFkZUC6M"
      },
      "outputs": [],
      "source": [
        "# conn.close()            #closing SQlite connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV0rtReKUC6M"
      },
      "outputs": [],
      "source": [
        "#saving dataset in CSV format\n",
        "# data.to_csv('Amazon-Fine-Food-Reviews.csv')\n",
        "\n",
        "#568454 rows are present in dataset.\n",
        "# checking count and no of distinct target outputs present in target column\n",
        "data['Score'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzGBnr6EUC6N"
      },
      "outputs": [],
      "source": [
        "count = data['Score'].value_counts()\n",
        "type(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ngOcXwwUC6N"
      },
      "outputs": [],
      "source": [
        "#Function for bar plot\n",
        "def bar_plot(x,y,width,xlabel,ylabel,label,color):\n",
        "    plt.bar(x,y,width=width,color=color,label=label)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6WF2STSUC6N"
      },
      "outputs": [],
      "source": [
        "bar_plot(count.keys(),count.values,0.25,'Score','Frequency','Frequency of each rating score','blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfkgG5RsUC6N"
      },
      "outputs": [],
      "source": [
        "# considering score > 3 as positive sentiment , <3 as negative sentiment and =3 as neutral.\n",
        "\n",
        "df = data[data['Score']!=3]  #Removing neutral reviews from the dataset\n",
        "print(df['Score'].value_counts())\n",
        "print(df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwadj4DVUC6O"
      },
      "outputs": [],
      "source": [
        "# % of data retained\n",
        "print((df.shape[0]*100)/data.shape[0],\"%\")\n",
        "val = df['Score'].value_counts()\n",
        "bar_plot(val.keys(),val.values,0.25,'Score','Frequency','Frequency of each rating score','green')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud7D8qavUC6P"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3d5SuDOUC6P"
      },
      "outputs": [],
      "source": [
        "len(df['UserId'].unique())  # no of unique userids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8-B1MV2UC6P"
      },
      "outputs": [],
      "source": [
        "# removing duplicates reviews from same user for a particular product\n",
        "df_new = df.drop_duplicates(subset=['ProductId','UserId','Text','ProfileName'], keep='first')\n",
        "df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPIEOduhUC6P"
      },
      "outputs": [],
      "source": [
        "print('No of duplicate reviews = ', df.shape[0]-df_new.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsaoxrjlUC6P"
      },
      "outputs": [],
      "source": [
        "# Helpfulness Numerator : Number of Peoples who found the review helpful to them.\n",
        "# Helpfulness Denominator : Number of Peoples indicated whether they found the review helpful or not.\n",
        "# Since helpfulness Numerator can't be more than Helpfull Denominator , we will be removing such record\n",
        "print(len(df_new[df_new['HelpfulnessNumerator']>df_new['HelpfulnessDenominator']]))\n",
        "\n",
        "#only 2 such records are present\n",
        "df_new = df_new[df_new['HelpfulnessNumerator']<=df_new['HelpfulnessDenominator']]\n",
        "print(df_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E25gr5rVUC6P"
      },
      "outputs": [],
      "source": [
        "#checking for null values\n",
        "print(df_new.isnull().any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ7_z751UC6Q"
      },
      "outputs": [],
      "source": [
        "# mapping score>3 as positive review [1] and score <3 as negative review [0]\n",
        "def re_score(x):\n",
        "    if x<3:\n",
        "        return 0\n",
        "    return 1\n",
        "\n",
        "df_new['Score'] = df_new['Score'].map(re_score)\n",
        "print(df_new['Score'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzmw81bwUC6Q"
      },
      "outputs": [],
      "source": [
        "count = df['Score'].value_counts()\n",
        "bar_plot(count.keys(),count.values,0.1,'Score','Frequency','Frequency of each rating score','blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H59JPGQPUC6Q"
      },
      "outputs": [],
      "source": [
        "xwee = df_new['Text'].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmyyf4fYUC6Q"
      },
      "source": [
        "Positive reviews = 442867\n",
        "Negative reviews = 81718\n",
        "\n",
        "Our dataset is a imbalanced dataset\n",
        "There are many methods of handling imbalanced dataset,we will be using SMOTE (an oversampling method that creates artificial instance id minority class by examining/selecting a random nearest neighbour by using KNN)\n",
        "\n",
        "Other way is to use tree based models as they handle imbalanced data in a better way than non-tree based models\n",
        "For metrics , we should use F1 score which depend on both precision and recall i.e. F1 score gets better when the amount and accuracy of predictions get better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41vf_5FOUC6Q"
      },
      "source": [
        "Resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBngzn5-UC6Q"
      },
      "outputs": [],
      "source": [
        "#For oversampling of dataset as we have an imbalance dataset\n",
        "\n",
        "def resampling(train_data,test_data):\n",
        "    smote = SMOTE(sampling_strategy='auto',random_state=42)\n",
        "    train_data_resampled , test_data_resampled = smote.fit_resample(train_data,test_data)\n",
        "    return train_data_resampled, test_data_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPkWso0QUC6Q"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjpjPRsDUC6R"
      },
      "outputs": [],
      "source": [
        "# Removing URLs from data\n",
        "def preprocessing_url(text):\n",
        "    text = re.sub(r'http\\S+','',text)\n",
        "    text = re.sub(r'www\\S+','',text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39xhJMIAUC6R"
      },
      "outputs": [],
      "source": [
        "# Removing HTML content from data\n",
        "def preprocessing_html(text):\n",
        "    text = BeautifulSoup(text).get_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqvX1FBkUC6R"
      },
      "outputs": [],
      "source": [
        "# Performing decontractions on data\n",
        "def decontraction(text):\n",
        "    text = re.sub(r\"won't\",\"will not\",text)\n",
        "    text = re.sub(r\"can't\",\"can not\",text)\n",
        "    text = re.sub(r\"n't\",\"not\",text)\n",
        "    text = re.sub(r\"\\'re\",\"are\",text)\n",
        "    text = re.sub(r\"\\'s\",\" is\",text)\n",
        "    text = re.sub(r\"\\'d\",\" would\",text)\n",
        "    text = re.sub(r\"\\'ll\",\" will\",text)\n",
        "    text = re.sub(r\"\\'t\",\" not\",text)\n",
        "    text = re.sub(r\"\\'ve\",\" have\",text)\n",
        "    text = re.sub(r\"\\'m\",\" am\",text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi3yKohjVRlV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuESdNsPUC6R"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QXNc2YyUC6R"
      },
      "outputs": [],
      "source": [
        "# defining stopwords vocabulary\n",
        "# stopwords = stopwords.words('english')\n",
        "# print(type(stopwords))\n",
        "\n",
        "# removing stopwords and lowering cases of words from dataset\n",
        "def remove_stopwords(text):\n",
        "\n",
        "    text = \" \".join([i.lower() for i in text.split() if i not in stop_words])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DwqMCAKUC6R"
      },
      "outputs": [],
      "source": [
        "# stemming words\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def stemming(text):\n",
        "    text = \" \".join([stemmer.stem(i) for i in text.split()])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saZUqxEEUC6R"
      },
      "outputs": [],
      "source": [
        "#Single Function for text preprocessing\n",
        "def text_preprocessing(df):\n",
        "    arr = []\n",
        "    for text in tqdm(df):\n",
        "        text = preprocessing_url(text)\n",
        "        text = preprocessing_html(text)\n",
        "        text = decontraction(text)\n",
        "        text = remove_stopwords(text)\n",
        "        text = stemming(text)\n",
        "        text = re.sub('\\S*\\d\\S*',\"\",text)    # removes any substrings from the string text that contain at least one digit surrounded by non-digit characters.\n",
        "        text = re.sub('[^A-Za-z0-9]+',\" \",text) # replaces any sequence of characters in the string s that is not a letter or a digit with a single space character \" \".\n",
        "        arr.append(text)\n",
        "\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IKs8GWsUC6S"
      },
      "outputs": [],
      "source": [
        "corpus = text_preprocessing(df_new['Text'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXdNdKP_UC6S"
      },
      "outputs": [],
      "source": [
        "abcd = corpus\n",
        "dataset = df_new.copy()\n",
        "dataset['Text'] = corpus\n",
        "dataset.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOngOuFUZ4Ef"
      },
      "outputs": [],
      "source": [
        "df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q41HLckdUC6S"
      },
      "outputs": [],
      "source": [
        "train_data , test_data = train_test_split(dataset,test_size=0.25, random_state=21)\n",
        "print(\"size of training data:\", len(train_data))\n",
        "print(\"size of test data:\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pHl1rHYUC6S"
      },
      "outputs": [],
      "source": [
        "# corpus = []\n",
        "\n",
        "# for text in tqdm(df_new['Text'].values):\n",
        "#     text = preprocessing_url(text)\n",
        "#     text = preprocessing_html(text)\n",
        "#     text = decontraction(text)\n",
        "#     text = remove_stopwords(text)\n",
        "#     text = stemming(text)\n",
        "#     text = re.sub('\\S*\\d\\S*',\"\",text)    # removes any substrings from the string text that contain at least one digit surrounded by non-digit characters.\n",
        "#     text = re.sub('[^A-Za-z0-9]+',\" \",text) # replaces any sequence of characters in the string s that is not a letter or a digit with a single space character \" \".\n",
        "#     corpus.append(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXQB1Ek2UC6S"
      },
      "source": [
        "Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHEMoWDgUC6T"
      },
      "source": [
        "1. Bag of Words (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doZ-dr8wUC6T"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(data):\n",
        "    bow = CountVectorizer()\n",
        "    return bow.fit(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgkNTLAjUC6a"
      },
      "source": [
        "2. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Scm359xUC6a"
      },
      "outputs": [],
      "source": [
        "def tf_idf(data):\n",
        "    tfidf = TfidfVectorizer()\n",
        "    return tfidf.fit(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V72FTS4UC6a"
      },
      "source": [
        "3. Average Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJttohbGUC6a"
      },
      "outputs": [],
      "source": [
        "def avg_w2v(arr):\n",
        "    ds = []\n",
        "    for word in arr:\n",
        "        ds.append(word.split())\n",
        "\n",
        "    model = Word2Vec(ds,vector_size=150,window=25,min_count=2)\n",
        "\n",
        "    avg_list = []\n",
        "    for i in tqdm(data):\n",
        "        vec= np.zeros(150)\n",
        "        count = 0\n",
        "        for j in i.split():\n",
        "            try:\n",
        "                vec+=model.wv[j]\n",
        "                count+=1\n",
        "            except:\n",
        "                pass\n",
        "        if count!=0:\n",
        "            vec = vec/count\n",
        "            avg_list.append(vec)\n",
        "        else:\n",
        "            avg_list.append(np.zeros(150))\n",
        "\n",
        "\n",
        "    print(\"Total no of vectors:\",len(avg_list))     #length of total no of vector\n",
        "    print(\"Dimension of vector:\",len(avg_list[1])) #length of avg vector\n",
        "\n",
        "    return np.array(avg_list)\n",
        "\n",
        "\n",
        "# for word2vec , data is required as list of lists  <--------------------------- IMP\n",
        "# more the data, better is the performance\n",
        "# http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XR0cft9fiXJ\n",
        "# size is number of dimensions of vector that you want to build\n",
        "# window is the maximum distance between two similar words\n",
        "# min_count is the minimum number of times the word must appear in corpus\n",
        "# workers is the numbers of cpu that you want to use"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5LEW5Njd4eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5szTEnHUC6a"
      },
      "outputs": [],
      "source": [
        "# w = \"hello my name is Abhishek\"\n",
        "# print(w.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54r3fJ_8UC6a"
      },
      "outputs": [],
      "source": [
        "def normalisation(data):\n",
        "    return Normalizer().fit_transform(data)\n",
        "\n",
        "def standardisation(data):\n",
        "    return StandardScaler().fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAqzYo9gUC6b"
      },
      "source": [
        "Building our Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yACGpQWpUC6b"
      },
      "outputs": [],
      "source": [
        "model_lr = LogisticRegression(max_iter=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTJhoEv4s6H_"
      },
      "source": [
        "Training on BOW model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qwd3yqI6uQw"
      },
      "outputs": [],
      "source": [
        "bow = bag_of_words(train_data['Text'])\n",
        "train_data_bow = bow.transform(train_data['Text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSYm3BOY9oTn"
      },
      "outputs": [],
      "source": [
        "print(\"Classification distribution before SMOTE\", Counter(train_data['Score']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SufseJ787HEQ"
      },
      "outputs": [],
      "source": [
        "# text data needs to be vectorized before USING smote\n",
        "#SMOTENC takes categorical data\n",
        "\n",
        "smote = SMOTE(sampling_strategy='auto',random_state=42)\n",
        "X_train_resampled , y_train_resampled = smote.fit_resample(train_data_bow,train_data['Score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxgI2xHG9sH9"
      },
      "outputs": [],
      "source": [
        "print('Classification distribtuion after SME', Counter(y_train_resampled))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk-Q2RLelcXH"
      },
      "outputs": [],
      "source": [
        "model_lr.fit(X_train_resampled,y_train_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGX7t_C3lxff"
      },
      "outputs": [],
      "source": [
        "test_data_bow = bow.transform(test_data['Text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B1reQeRm8yd"
      },
      "outputs": [],
      "source": [
        "y_pred = model_lr.predict(test_data_bow)\n",
        "score = accuracy_score(y_pred,test_data['Score']) *100\n",
        "\n",
        "print(\"Accuracy acheived by Logistic Regression using bow =\",score,\"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxfwHCecnU-U"
      },
      "outputs": [],
      "source": [
        "X_train_resampled_n = normalisation(X_train_resampled)\n",
        "test_data_bow_n = normalisation(test_data_bow)\n",
        "model_lr.fit(X_train_resampled_n,y_train_resampled)\n",
        "y_pred = model_lr.predict(test_data_bow_n)\n",
        "score = accuracy_score(y_pred,test_data['Score']) *100\n",
        "print(\"Accuracy acheived by Logistic Regression using bow after normalisation =\",score,\"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfqdG058s_Ei"
      },
      "source": [
        "Training on TF-IDF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuukPCEaoFjM"
      },
      "outputs": [],
      "source": [
        "tfidf=tf_idf(train_data['Text'])\n",
        "train_data_tfidf = tfidf.transform(train_data['Text'])\n",
        "\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(train_data_tfidf, train_data['Score'])\n",
        "model_lr.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "test_data_tfidf = tfidf.transform(test_data['Text'])\n",
        "y_pred = model_lr.predict(test_data_tfidf)\n",
        "\n",
        "score = accuracy_score(y_pred, test_data['Score']) *100\n",
        "print(\"Accuracy acheived by Logistic Regression using TF-IDF =\",score,\"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training on w2v model"
      ],
      "metadata": {
        "id": "jZraDDIsfeSb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZyUSZI7BA2fp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}